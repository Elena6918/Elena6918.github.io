<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Minjun (Elena) | Understanding Deepfake Video Detection: From Signals to Synchrony</title> <meta name="author" content="Minjun (Elena) Long"> <meta name="description" content="A literature overview (as of year 2025) of Deepfake video detection methods, combined with my personal research journey: what I explored, what I learned, and ideas that I like but ultimately didn’t publish. This post aims to help newcomers grasp the field’s landscape and challenges, while offering reflections on ideas, failures, and future directions."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%95&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://elena6918.github.io/blog/2025/deepfake/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Minjun (Elena) </span>Long</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Understanding Deepfake Video Detection: From Signals to Synchrony</h1> <p class="post-meta">July 11, 2025</p> <p class="post-tags"> <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/research"> <i class="fas fa-hashtag fa-sm"></i> research</a>     ·   <a href="/blog/category/academic"> <i class="fas fa-tag fa-sm"></i> academic</a>   </p> </header> <article class="post-content"> <h2 id="what-is-a-deepfake-video">What Is a Deepfake Video?</h2> <p>In this post, I define a deepfake video as any audiovisual content that has been manipulated to impersonate another person. This manipulation can occur in the visual domain, such as face swapping, reenactment, or model-based synthesis; or in the audio domain, via text-to-speech (TTS) voice cloning or voice conversion.</p> <p>While deepfakes can be entertaining or artistic, the real concern arises when they’re used for impersonation, misinformation, or character assassination. This raises an urgent need for detection methods that are both effective and generalizable.</p> <p><br></p> <h2 id="a-quick-guide-what-youll-find-in-this-post">A Quick Guide: What You’ll Find in This Post</h2> <p>This post is both a literature review and a personal research narrative. If you’re new to deepfake detection or want to skip to what’s most relevant, here’s a quick roadmap:</p> <ul> <li> <strong><a href="#unimodal-deepfake-detection">Unimodal Detection</a></strong> — Audio or video-based only</li> <li> <strong><a href="#temporal-modeling-in-deepfake-detection">Temporal Modeling</a></strong> — Consistency across time</li> <li> <strong><a href="#revisiting-audio-visual-deepfake-detection">Audio-Visual Detection</a></strong> — Synchrony, fusion, and biometric alignment</li> <li> <strong><a href="#reflections-on-my-own-experiments">My Research Reflections</a></strong> — What I tried, what failed, and where I see promise</li> <li> <strong><a href="#final-thoughts">Final Thoughts</a></strong> — What might actually give defenders an edge</li> </ul> <p><br></p> <h2 id="unimodal-deepfake-detection">Unimodal Deepfake Detection</h2> <h3 id="video-based-detection">Video-Based Detection</h3> <p>Early deepfake detectors emerged from digital forensics, targeting signals that generative models typically fail to capture—like physiological cues and geometric constraints.</p> <p>Examples include:</p> <ul> <li><a href="https://ieeexplore.ieee.org/document/9072088" rel="external nofollow noopener" target="_blank">Eye-blinking patterns</a></li> <li><a href="https://arxiv.org/abs/2110.15561" rel="external nofollow noopener" target="_blank">Heart rate estimation from face</a></li> <li><a href="https://ieeexplore.ieee.org/document/9465076" rel="external nofollow noopener" target="_blank">Landmark geometry inconsistencies</a></li> </ul> <p>As deepfakes got better, frame-level forensic detectors appeared, followed by models analyzing <strong>motion</strong> across frames, such as <a href="https://ieeexplore.ieee.org/document/9022558" rel="external nofollow noopener" target="_blank">optical flow</a> or <a href="https://arxiv.org/abs/2109.01860" rel="external nofollow noopener" target="_blank">temporal consistency</a>.</p> <p><br></p> <h3 id="audio-based-detection">Audio-Based Detection</h3> <p>Audio deepfakes introduce their own telltale traces, either in signal properties or in the way they’re generated.</p> <p>Approaches include:</p> <ul> <li> <a href="https://arxiv.org/abs/2009.01934" rel="external nofollow noopener" target="_blank">Frequency-based methods</a> like bispectral analysis</li> <li><a href="https://arxiv.org/abs/2404.15143v2" rel="external nofollow noopener" target="_blank">Breathing pattern analysis</a></li> <li><a href="https://arxiv.org/abs/2402.18085" rel="external nofollow noopener" target="_blank">Liveness testing via challenge-response</a></li> <li><a href="https://arxiv.org/abs/2308.14970" rel="external nofollow noopener" target="_blank">Embedding-based classifiers</a></li> <li><a href="https://arxiv.org/abs/2304.13085" rel="external nofollow noopener" target="_blank">Vocoder fingerprinting</a></li> </ul> <p><br></p> <h2 id="temporal-modeling-in-deepfake-detection">Temporal Modeling in Deepfake Detection</h2> <p>Another key axis for classification is <strong>temporal modeling</strong>. Even high-quality fakes often fail to maintain consistency across frames.</p> <p>Temporal methods fall into two groups:</p> <ul> <li> <strong>Artifact-focused</strong>: Detect generation noise or device-specific patterns.</li> <li> <strong>Identity-focused</strong>: Track how personal characteristics evolve over time.</li> </ul> <p><br></p> <h3 id="artifact-focused-temporal-modeling">Artifact-Focused Temporal Modeling</h3> <p>These models detect inconsistencies from compression or camera-specific signals:</p> <ul> <li><a href="https://arxiv.org/abs/2506.20548" rel="external nofollow noopener" target="_blank">Video compression artifacts</a></li> <li><a href="https://arxiv.org/abs/2310.20621" rel="external nofollow noopener" target="_blank">Camera model noise patterns</a></li> </ul> <p>A standout method is <a href="https://ieeexplore.ieee.org/document/10030936" rel="external nofollow noopener" target="_blank">TI²Net</a>, which compares face-identity embeddings over time. Using a recurrent model and triplet loss, it learns to spot identity fluctuations between frames, something that shouldn’t happen in real footage.</p> <p><br></p> <h3 id="identity-focused-temporal-modeling">Identity-Focused Temporal Modeling</h3> <p>These methods track stable identity traits and compare them across time or against a reference.</p> <ul> <li> <a href="https://ieeexplore.ieee.org/document/9710044" rel="external nofollow noopener" target="_blank">ID-Reveal</a>: Uses 3DMM geometry and GAN-based learning for speaker-dependent verification.</li> <li> <a href="https://arxiv.org/abs/2004.14491" rel="external nofollow noopener" target="_blank">Appearance + Behavior decomposition</a>: Separates visual identity from motion patterns like facial expressions and head pose.</li> </ul> <p>These models often work best with speaker-specific reference videos and build strong personalized detectors.</p> <p><br></p> <h2 id="revisiting-audio-visual-deepfake-detection">Revisiting Audio-Visual Deepfake Detection</h2> <p>Let’s now focus on the intersection of audio and video — the most promising and complex area in detection (in my opinion).</p> <p>Audio-visual deepfake detectors leverage the synchrony between speech and visual appearance. Depending on how features are extracted and compared, they fall into three broad categories:</p> <p><br></p> <h3 id="1-synchronization-based-methods-low-level">1. Synchronization-Based Methods (Low-Level)</h3> <p>These methods focus purely on timing: does the mouth move when the voice says something?</p> <ul> <li> <a href="https://arxiv.org/abs/2301.01767" rel="external nofollow noopener" target="_blank">Self-Supervised Video Forensics</a>: Measures delay between phoneme onsets and lip movement.</li> <li> <a href="https://ieeexplore.ieee.org/document/9710387" rel="external nofollow noopener" target="_blank">Joint AV Deepfake Detection</a>: Trains modality-specific detectors, then aligns them using attention mechanisms.</li> </ul> <p>They’re fast but don’t model speaker traits or expression dynamics.</p> <p><br></p> <h3 id="2-fusion-based-methods-low-to-high-level">2. Fusion-Based Methods (Low to High-Level)</h3> <p>These models fuse audio and video features into a joint representation using neural networks, usually guided by contrastive loss.</p> <p><strong>Low-level fusion</strong> (timing + surface-level correlation):</p> <ul> <li><a href="https://arxiv.org/abs/2005.14405" rel="external nofollow noopener" target="_blank">Not Made for Each Other</a></li> <li><a href="https://dl.acm.org/doi/10.1145/3625100" rel="external nofollow noopener" target="_blank">Joint AV Attention with Contrastive Learning</a></li> </ul> <p><strong>Mid- to high-level fusion</strong> (emotion, identity, semantics):</p> <ul> <li> <a href="https://arxiv.org/abs/2003.06711" rel="external nofollow noopener" target="_blank">Emotions Don’t Lie</a>: Cross-checks emotion consistency across modalities.</li> <li> <a href="https://arxiv.org/abs/2203.02195" rel="external nofollow noopener" target="_blank">Voice-Face Homogeneity</a>: Uses contrastive loss to align speaker identity from voice and face.</li> </ul> <p>Fusion models are powerful but less interpretable than synchronization models.</p> <p><br></p> <h3 id="3-cross-modal-biometric-matching">3. Cross-Modal Biometric Matching</h3> <p>Here, the goal is to determine: does this face belong to this voice?</p> <ul> <li> <a href="https://arxiv.org/abs/1905.09773" rel="external nofollow noopener" target="_blank">Speech2Face</a> and <a href="https://arxiv.org/abs/1804.00326" rel="external nofollow noopener" target="_blank">Seeing Voices</a>: Show that voice and face carry latent demographic information.</li> <li> <a href="https://arxiv.org/abs/2203.02195" rel="external nofollow noopener" target="_blank">Voice-Face Homogeneity Tells Deepfake</a>: Builds on this to detect mismatches in fakes.</li> </ul> <p>These models work well in identity verification settings but may struggle with unknown speakers.</p> <p><br></p> <h2 id="what-contrastive-learning-actually-learns">What Contrastive Learning Actually Learns</h2> <p>Contrastive loss is everywhere — but what does it <em>actually</em> align?</p> <p>Depending on the dataset, architecture, and training signals, the model might be learning any of the following:</p> <ul> <li>Emotional consistency (e.g., smiles with happy tone)</li> <li>Identity-related traits (e.g., facial structure ↔ timbre)</li> <li>Timing (e.g., lip closures and consonants)</li> <li>Vocal tract constraints (e.g., shape → formants)</li> </ul> <p>Most fusion-based approaches mentioned earlier implicitly assume at least one of these correlations, or try to capture them in a data-driven way. Some methods explicitly focus on the first three (emotion, identity, timing), while others attempt to unpack the final one: anatomical alignment.</p> <p>For example:</p> <ul> <li> <a href="https://dl.acm.org/doi/10.1016/j.cviu.2024.104133" rel="external nofollow noopener" target="_blank">AV Articulatory Learning</a>: Trains on vocal tract variables and lip motion.</li> <li> <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/blue" rel="external nofollow noopener" target="_blank">Who Are You</a>: Reconstructs vocal tract from audio as a biometric.</li> </ul> <p>These ideas were a major inspiration for my own experiments. Even though my approach didn’t work out as hoped, it shaped how I think about what contrastive learning might actually be encoding, and what makes some of these signals easier to fake than others.</p> <p><br></p> <h2 id="reflections-on-my-own-experiments">Reflections on My Own Experiments</h2> <hr> <h3 id="my-hypothesis">My Hypothesis</h3> <p>Inspired by the articulatory literature, I tried to learn a <strong>joint representation</strong> from audio and visual <strong>motion features</strong>: deltas of audio features extracted with OpenSmile and optical flow.</p> <p>I thought: instead of high-level semantics or identity, maybe we can just track movement synchrony — speech-induced motion — between lips and voice.</p> <h3 id="what-went-wrong">What Went Wrong</h3> <p>The model didn’t learn a joint space. Each modality formed its own subspace, and synchronization cues weren’t shared. The representation was not truly joint, likely because the design lacked an objective to encourage <strong>mutual dependence</strong> or <strong>cross-modal supervision</strong>. Therefore, there was no incentive to use one modality to help interpret the other.</p> <p>Further down the path, I also experimented with two additional tasks:</p> <ol> <li> <strong>Cross-modal reconstruction</strong>: using audio to reconstruct video features, or vice versa</li> <li> <strong>Motion-based segment matching</strong>: determining whether a given audio snippet corresponds to a visual motion segment</li> </ol> <p>Both attempts failed, likely due to the limitations of using a small, general-purpose audio-visual dataset. For example, the same event label (like “bell ring”) can appear with vastly different sounds and visual contexts, making it hard for the model to learn consistent cross-modal correspondences.</p> <p>In hindsight, this line of work is more aligned with <strong>audio-visual event localization</strong>, where aligning semantics across modalities is already known to be challenging.</p> <p>Moreover, cross-modal reconstruction may simply be too difficult on general datasets. While audio and visual motion in speech are tightly correlated due to shared underlying articulatory dynamics, general events lack that structure. Learning robust semantic representations in this setting would likely require much larger datasets with many instances per class — otherwise, the model risks overfitting or collapsing into shortcut features.</p> <p><br></p> <h3 id="ideas-i-didnt-pursue-but-still-like">Ideas I Didn’t Pursue (but Still Like)</h3> <p>Either of them requires pre-training on a large dataset.</p> <p><strong>1. Speaking Style Modeling</strong></p> <p>Instead of phoneme-level synchrony, model <strong>speaking style</strong> over longer clips (5 to 10 seconds).</p> <ul> <li>Temporal rhythm, prosody, gesture intensity</li> <li>Contrastive learning could group similar styles together</li> <li>May reveal differences that deepfakes can’t mimic</li> </ul> <p><strong>2. Vocal Tract Biometrics</strong></p> <p>Inspired by <a href="https://dl.acm.org/doi/10.1145/3658644.3670358" rel="external nofollow noopener" target="_blank">SAFARI</a>, I imagined disentangling speaker physiology from speech content:</p> <ul> <li>Stable: vocal tract shape, articulation range</li> <li>Variable: phoneme identity, intonation</li> </ul> <p>If we can extract a <strong>constant vocal tract representation</strong>, we could match that across time, even as phonemes change. This would be extremely hard for generative models to fake convincingly.</p> <p><br></p> <h2 id="final-thoughts">Final Thoughts</h2> <p>The central challenge in this arms race is identifying an <strong>asymmetry</strong> that gives defenders an advantage. If both the attacker and defender are training models, what gives the defender more leverage?</p> <p>One promising answer lies in identity-based detection, where defenders can provide authentic reference videos at test time. In this setting, the defender may possess more data about the true subject than the attacker, enabling stronger verification.</p> <p>Another, perhaps more fundamental, advantage lies in physiology and physical constraints. The human vocal tract, facial musculature, and speaking behavior are highly complex and deeply individualized — too intricate to be fully modeled by current generative systems. Yet these physiological signatures may be simpler to <strong>verify</strong> than to convincingly <strong>synthesize</strong>.</p> <p>Until policy, law, and society catch up, technical defenses are our first line. Developing ensemble systems, some modeling artifacts, others modeling synchrony or biometrics, offer layered protection.</p> <p>And they serve another role: helping people prove that a real video wasn’t fake, or vice versa. In a world where doubt can be weaponized, robust detection isn’t just technical infrastructure — it’s social infrastructure.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2026 Minjun (Elena) Long. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: February 08, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>