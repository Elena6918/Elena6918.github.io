<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://elena6918.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://elena6918.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-08T13:54:49-05:00</updated><id>https://elena6918.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Understanding Deepfake Video Detection: From Signals to Synchrony</title><link href="https://elena6918.github.io/blog/2025/deepfake/" rel="alternate" type="text/html" title="Understanding Deepfake Video Detection: From Signals to Synchrony"/><published>2025-07-11T00:00:00-04:00</published><updated>2025-07-11T00:00:00-04:00</updated><id>https://elena6918.github.io/blog/2025/deepfake</id><content type="html" xml:base="https://elena6918.github.io/blog/2025/deepfake/"><![CDATA[<h2 id="what-is-a-deepfake-video">What Is a Deepfake Video?</h2> <p>In this post, I define a deepfake video as any audiovisual content that has been manipulated to impersonate another person. This manipulation can occur in the visual domain, such as face swapping, reenactment, or model-based synthesis; or in the audio domain, via text-to-speech (TTS) voice cloning or voice conversion.</p> <p>While deepfakes can be entertaining or artistic, the real concern arises when they’re used for impersonation, misinformation, or character assassination. This raises an urgent need for detection methods that are both effective and generalizable.</p> <p><br/></p> <h2 id="a-quick-guide-what-youll-find-in-this-post">A Quick Guide: What You’ll Find in This Post</h2> <p>This post is both a literature review and a personal research narrative. If you’re new to deepfake detection or want to skip to what’s most relevant, here’s a quick roadmap:</p> <ul> <li><strong><a href="#unimodal-deepfake-detection">Unimodal Detection</a></strong> — Audio or video-based only</li> <li><strong><a href="#temporal-modeling-in-deepfake-detection">Temporal Modeling</a></strong> — Consistency across time</li> <li><strong><a href="#revisiting-audio-visual-deepfake-detection">Audio-Visual Detection</a></strong> — Synchrony, fusion, and biometric alignment</li> <li><strong><a href="#reflections-on-my-own-experiments">My Research Reflections</a></strong> — What I tried, what failed, and where I see promise</li> <li><strong><a href="#final-thoughts">Final Thoughts</a></strong> — What might actually give defenders an edge</li> </ul> <p><br/></p> <h2 id="unimodal-deepfake-detection">Unimodal Deepfake Detection</h2> <h3 id="video-based-detection">Video-Based Detection</h3> <p>Early deepfake detectors emerged from digital forensics, targeting signals that generative models typically fail to capture—like physiological cues and geometric constraints.</p> <p>Examples include:</p> <ul> <li><a href="https://ieeexplore.ieee.org/document/9072088">Eye-blinking patterns</a></li> <li><a href="https://arxiv.org/abs/2110.15561">Heart rate estimation from face</a></li> <li><a href="https://ieeexplore.ieee.org/document/9465076">Landmark geometry inconsistencies</a></li> </ul> <p>As deepfakes got better, frame-level forensic detectors appeared, followed by models analyzing <strong>motion</strong> across frames, such as <a href="https://ieeexplore.ieee.org/document/9022558">optical flow</a> or <a href="https://arxiv.org/abs/2109.01860">temporal consistency</a>.</p> <p><br/></p> <h3 id="audio-based-detection">Audio-Based Detection</h3> <p>Audio deepfakes introduce their own telltale traces, either in signal properties or in the way they’re generated.</p> <p>Approaches include:</p> <ul> <li><a href="https://arxiv.org/abs/2009.01934">Frequency-based methods</a> like bispectral analysis</li> <li><a href="https://arxiv.org/abs/2404.15143v2">Breathing pattern analysis</a></li> <li><a href="https://arxiv.org/abs/2402.18085">Liveness testing via challenge-response</a></li> <li><a href="https://arxiv.org/abs/2308.14970">Embedding-based classifiers</a></li> <li><a href="https://arxiv.org/abs/2304.13085">Vocoder fingerprinting</a></li> </ul> <p><br/></p> <h2 id="temporal-modeling-in-deepfake-detection">Temporal Modeling in Deepfake Detection</h2> <p>Another key axis for classification is <strong>temporal modeling</strong>. Even high-quality fakes often fail to maintain consistency across frames.</p> <p>Temporal methods fall into two groups:</p> <ul> <li><strong>Artifact-focused</strong>: Detect generation noise or device-specific patterns.</li> <li><strong>Identity-focused</strong>: Track how personal characteristics evolve over time.</li> </ul> <p><br/></p> <h3 id="artifact-focused-temporal-modeling">Artifact-Focused Temporal Modeling</h3> <p>These models detect inconsistencies from compression or camera-specific signals:</p> <ul> <li><a href="https://arxiv.org/abs/2506.20548">Video compression artifacts</a></li> <li><a href="https://arxiv.org/abs/2310.20621">Camera model noise patterns</a></li> </ul> <p>A standout method is <a href="https://ieeexplore.ieee.org/document/10030936">TI²Net</a>, which compares face-identity embeddings over time. Using a recurrent model and triplet loss, it learns to spot identity fluctuations between frames, something that shouldn’t happen in real footage.</p> <p><br/></p> <h3 id="identity-focused-temporal-modeling">Identity-Focused Temporal Modeling</h3> <p>These methods track stable identity traits and compare them across time or against a reference.</p> <ul> <li><a href="https://ieeexplore.ieee.org/document/9710044">ID-Reveal</a>: Uses 3DMM geometry and GAN-based learning for speaker-dependent verification.</li> <li><a href="https://arxiv.org/abs/2004.14491">Appearance + Behavior decomposition</a>: Separates visual identity from motion patterns like facial expressions and head pose.</li> </ul> <p>These models often work best with speaker-specific reference videos and build strong personalized detectors.</p> <p><br/></p> <h2 id="revisiting-audio-visual-deepfake-detection">Revisiting Audio-Visual Deepfake Detection</h2> <p>Let’s now focus on the intersection of audio and video — the most promising and complex area in detection (in my opinion).</p> <p>Audio-visual deepfake detectors leverage the synchrony between speech and visual appearance. Depending on how features are extracted and compared, they fall into three broad categories:</p> <p><br/></p> <h3 id="1-synchronization-based-methods-low-level">1. Synchronization-Based Methods (Low-Level)</h3> <p>These methods focus purely on timing: does the mouth move when the voice says something?</p> <ul> <li><a href="https://arxiv.org/abs/2301.01767">Self-Supervised Video Forensics</a>: Measures delay between phoneme onsets and lip movement.</li> <li><a href="https://ieeexplore.ieee.org/document/9710387">Joint AV Deepfake Detection</a>: Trains modality-specific detectors, then aligns them using attention mechanisms.</li> </ul> <p>They’re fast but don’t model speaker traits or expression dynamics.</p> <p><br/></p> <h3 id="2-fusion-based-methods-low-to-high-level">2. Fusion-Based Methods (Low to High-Level)</h3> <p>These models fuse audio and video features into a joint representation using neural networks, usually guided by contrastive loss.</p> <p><strong>Low-level fusion</strong> (timing + surface-level correlation):</p> <ul> <li><a href="https://arxiv.org/abs/2005.14405">Not Made for Each Other</a></li> <li><a href="https://dl.acm.org/doi/10.1145/3625100">Joint AV Attention with Contrastive Learning</a></li> </ul> <p><strong>Mid- to high-level fusion</strong> (emotion, identity, semantics):</p> <ul> <li><a href="https://arxiv.org/abs/2003.06711">Emotions Don’t Lie</a>: Cross-checks emotion consistency across modalities.</li> <li><a href="https://arxiv.org/abs/2203.02195">Voice-Face Homogeneity</a>: Uses contrastive loss to align speaker identity from voice and face.</li> </ul> <p>Fusion models are powerful but less interpretable than synchronization models.</p> <p><br/></p> <h3 id="3-cross-modal-biometric-matching">3. Cross-Modal Biometric Matching</h3> <p>Here, the goal is to determine: does this face belong to this voice?</p> <ul> <li><a href="https://arxiv.org/abs/1905.09773">Speech2Face</a> and <a href="https://arxiv.org/abs/1804.00326">Seeing Voices</a>: Show that voice and face carry latent demographic information.</li> <li><a href="https://arxiv.org/abs/2203.02195">Voice-Face Homogeneity Tells Deepfake</a>: Builds on this to detect mismatches in fakes.</li> </ul> <p>These models work well in identity verification settings but may struggle with unknown speakers.</p> <p><br/></p> <h2 id="what-contrastive-learning-actually-learns">What Contrastive Learning Actually Learns</h2> <p>Contrastive loss is everywhere — but what does it <em>actually</em> align?</p> <p>Depending on the dataset, architecture, and training signals, the model might be learning any of the following:</p> <ul> <li>Emotional consistency (e.g., smiles with happy tone)</li> <li>Identity-related traits (e.g., facial structure ↔ timbre)</li> <li>Timing (e.g., lip closures and consonants)</li> <li>Vocal tract constraints (e.g., shape → formants)</li> </ul> <p>Most fusion-based approaches mentioned earlier implicitly assume at least one of these correlations, or try to capture them in a data-driven way. Some methods explicitly focus on the first three (emotion, identity, timing), while others attempt to unpack the final one: anatomical alignment.</p> <p>For example:</p> <ul> <li><a href="https://dl.acm.org/doi/10.1016/j.cviu.2024.104133">AV Articulatory Learning</a>: Trains on vocal tract variables and lip motion.</li> <li><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/blue">Who Are You</a>: Reconstructs vocal tract from audio as a biometric.</li> </ul> <p>These ideas were a major inspiration for my own experiments. Even though my approach didn’t work out as hoped, it shaped how I think about what contrastive learning might actually be encoding, and what makes some of these signals easier to fake than others.</p> <p><br/></p> <h2 id="reflections-on-my-own-experiments">Reflections on My Own Experiments</h2> <hr/> <h3 id="my-hypothesis">My Hypothesis</h3> <p>Inspired by the articulatory literature, I tried to learn a <strong>joint representation</strong> from audio and visual <strong>motion features</strong>: deltas of audio features extracted with OpenSmile and optical flow.</p> <p>I thought: instead of high-level semantics or identity, maybe we can just track movement synchrony — speech-induced motion — between lips and voice.</p> <h3 id="what-went-wrong">What Went Wrong</h3> <p>The model didn’t learn a joint space. Each modality formed its own subspace, and synchronization cues weren’t shared. The representation was not truly joint, likely because the design lacked an objective to encourage <strong>mutual dependence</strong> or <strong>cross-modal supervision</strong>. Therefore, there was no incentive to use one modality to help interpret the other.</p> <p>Further down the path, I also experimented with two additional tasks:</p> <ol> <li><strong>Cross-modal reconstruction</strong>: using audio to reconstruct video features, or vice versa</li> <li><strong>Motion-based segment matching</strong>: determining whether a given audio snippet corresponds to a visual motion segment</li> </ol> <p>Both attempts failed, likely due to the limitations of using a small, general-purpose audio-visual dataset. For example, the same event label (like “bell ring”) can appear with vastly different sounds and visual contexts, making it hard for the model to learn consistent cross-modal correspondences.</p> <p>In hindsight, this line of work is more aligned with <strong>audio-visual event localization</strong>, where aligning semantics across modalities is already known to be challenging.</p> <p>Moreover, cross-modal reconstruction may simply be too difficult on general datasets. While audio and visual motion in speech are tightly correlated due to shared underlying articulatory dynamics, general events lack that structure. Learning robust semantic representations in this setting would likely require much larger datasets with many instances per class — otherwise, the model risks overfitting or collapsing into shortcut features.</p> <p><br/></p> <h3 id="ideas-i-didnt-pursue-but-still-like">Ideas I Didn’t Pursue (but Still Like)</h3> <p>Either of them requires pre-training on a large dataset.</p> <p><strong>1. Speaking Style Modeling</strong></p> <p>Instead of phoneme-level synchrony, model <strong>speaking style</strong> over longer clips (5 to 10 seconds).</p> <ul> <li>Temporal rhythm, prosody, gesture intensity</li> <li>Contrastive learning could group similar styles together</li> <li>May reveal differences that deepfakes can’t mimic</li> </ul> <p><strong>2. Vocal Tract Biometrics</strong></p> <p>Inspired by <a href="https://dl.acm.org/doi/10.1145/3658644.3670358">SAFARI</a>, I imagined disentangling speaker physiology from speech content:</p> <ul> <li>Stable: vocal tract shape, articulation range</li> <li>Variable: phoneme identity, intonation</li> </ul> <p>If we can extract a <strong>constant vocal tract representation</strong>, we could match that across time, even as phonemes change. This would be extremely hard for generative models to fake convincingly.</p> <p><br/></p> <h2 id="final-thoughts">Final Thoughts</h2> <p>The central challenge in this arms race is identifying an <strong>asymmetry</strong> that gives defenders an advantage. If both the attacker and defender are training models, what gives the defender more leverage?</p> <p>One promising answer lies in identity-based detection, where defenders can provide authentic reference videos at test time. In this setting, the defender may possess more data about the true subject than the attacker, enabling stronger verification.</p> <p>Another, perhaps more fundamental, advantage lies in physiology and physical constraints. The human vocal tract, facial musculature, and speaking behavior are highly complex and deeply individualized — too intricate to be fully modeled by current generative systems. Yet these physiological signatures may be simpler to <strong>verify</strong> than to convincingly <strong>synthesize</strong>.</p> <p>Until policy, law, and society catch up, technical defenses are our first line. Developing ensemble systems, some modeling artifacts, others modeling synchrony or biometrics, offer layered protection.</p> <p>And they serve another role: helping people prove that a real video wasn’t fake, or vice versa. In a world where doubt can be weaponized, robust detection isn’t just technical infrastructure — it’s social infrastructure.</p>]]></content><author><name></name></author><category term="academic"/><category term="research"/><summary type="html"><![CDATA[A literature overview (as of year 2025) of Deepfake video detection methods, combined with my personal research journey: what I explored, what I learned, and ideas that I like but ultimately didn’t publish. This post aims to help newcomers grasp the field’s landscape and challenges, while offering reflections on ideas, failures, and future directions.]]></summary></entry><entry><title type="html">Understanding Optical Flow Visualization</title><link href="https://elena6918.github.io/blog/2025/opticalflow/" rel="alternate" type="text/html" title="Understanding Optical Flow Visualization"/><published>2025-01-24T17:39:00-05:00</published><updated>2025-01-24T17:39:00-05:00</updated><id>https://elena6918.github.io/blog/2025/opticalflow</id><content type="html" xml:base="https://elena6918.github.io/blog/2025/opticalflow/"><![CDATA[]]></content><author><name></name></author><category term="academic"/><category term="research"/><summary type="html"><![CDATA[My medium post on understanding optical flow visualization as part of the research for audio-visual Deepfake video detection.]]></summary></entry><entry><title type="html">Profile Pictures</title><link href="https://elena6918.github.io/blog/2022/drawing/" rel="alternate" type="text/html" title="Profile Pictures"/><published>2022-02-25T00:00:00-05:00</published><updated>2022-02-25T00:00:00-05:00</updated><id>https://elena6918.github.io/blog/2022/drawing</id><content type="html" xml:base="https://elena6918.github.io/blog/2022/drawing/"><![CDATA[<p>A collection of Past Profile Pictures. Starting from 2022, I have decided to draw one self-protrait with the Chinese Zodiac theme.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic_22-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic_22-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic_22-1400.webp"/> <img src="/assets/img/prof_pic_22.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The Year of Tiger (2022)</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic_23-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic_23-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic_23-1400.webp"/> <img src="/assets/img/prof_pic_23.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The Year of Rabbit (2023)</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic_24-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic_24-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic_24-1400.webp"/> <img src="/assets/img/prof_pic_24.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The Year of Dragon (2024)</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic_25-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic_25-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic_25-1400.webp"/> <img src="/assets/img/prof_pic_25.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The Year of Snake (2025)</p>]]></content><author><name></name></author><category term="fun"/><category term="images"/><summary type="html"><![CDATA[An archive for my past profile pictures.]]></summary></entry></feed>